BEP-21-Synaptic dynamics

Abstract:
	Currently, synaptic dynamics are only handled at the pre or post-synaptic
	neuron, rather than at the local synapse. For example, for a given
	postsynaptic neuron, there is a single
	equation for all synapses with the same dynamics (e.g. AMPA excitatory
	current) and all presynaptic spikes impact the same variable. In STDP,
	synaptic variables are considered as either presynaptic or postsynaptic,
	not specific of each synapse.
	This design implies strong constraint on synaptic models (kinetics
	and plasticity).
	This BEP aims at overcoming these limitations.

Missing features and a few ideas
================================
* Nonlinear synapses (e.g. NMDA). These require one conductance variable per
  synapse, and separate dynamics. The idea is to
  define differential equations on synaptic weights.
  We could use a fake NeuronGroup as a proxy to synaptic weights + state updater.
  We could use a slower clock to speed up the simulation.
  Issues: 1) several synaptic variables, 2) heterogeneous delays, 3) routing
  spikes (now presynaptic neurons act on different variables).
  A priori, it is not possible to do event-driven dynamics, because of the
  interaction of the conductances with the membrane equation. But maybe there is
  a trick to be found.

* Probabilistic synapses.
  The main difficulty here is to integrate it with STP and STDP.

* Multiple synapses (with different delays) between the same pre/post neurons
  This is almost done, except we need to check if it works with STDP.
  We also need to think of a nice syntax.
  
* STDP: access to postsynaptic variables
  This is probably not a major problem (and can be done with the current version,
  although not in an elegant way), unless it is coupled with local synaptic
  variables. We have one example (Fusi's STDP rule) from people in the mailing
  list.

* STDP: rules with local synaptic variables
  Here we need event-driven dynamics. But one issue is dealing with heterogeneous
  delays. We should start with an existing example (STDP1/2). Then the most
  interesting rule is Graupner/Brunel's.
  
* Neuromodulation. There is a question on this in the Brian support list. This
  is also something that NEST supports. There are two issues here: 1) continuous
  weight dynamics, which should be computed with event-driven updates, 2)
  modulation by an external signal, which is a trace of spikes from other neurons.
  At this point, I'm familiar enough with this to address point 2.

The main technical difficulty, I think, is dealing with heterogeneous delays,
which might require event queues.

Multiple synapses
-----------------
Having multiple synapses (for a given pre/post pair) is entirely possible
with the current sparse connection structure (although STDP should be checked).
What is really needed is a nice syntax to create these synapses and
access the weights.

Nonlinear synapses
------------------
The classic example is NMDA synapses.
Reference:
Probabilistic decision making by slow reverberation in cortical circuits.
XJ Wang, Neuron 2002.

Model:
dg/dt=-a*g+b*x*(1-g)
dx/dt=-c*x
and spikes act on x (addition).

The conductance then enters the membrane equation.
The problem here is that it is a nonlinear system (product of x and g),
and therefore we cannot simply simulate the total conductance, as we do
with linear synapses ("lumped variable"). Each synapse must be individually
simulated. This implies a huge computational cost (proportional to
number of synapses/dt for each second). It cannot be event-driven because the
value of the total conductance is needed at each time step.

There is an example in the support list, where people used a NetworkOperation
to do it.

Tricks
^^^^^^
Example x(t) should be close to 0 most of the time, because 1/c represents the
rise time, which is short. When x=0, the equations are linear (linear decay
of g), and therefore we only need to simulate the lumped variable gtot(t)
corresponding to the sum of all conductances for which x=0. This could however be
quite painful to code.

Another approach is to use an approximation of NMDA dynamics:
dg/dt=-a*g+b*x
dx/dt=-c*x
and x->x+(1-g)*w

This makes sense because g(t) changes slowly relatively to x(t).
If we do that, the equations are linear and we only need to simulate
the lumped variables. However, we still need individual synaptic values
for g (not lumped) at spike time. Here we could use event-driven updates
of g - this is possible because the equations are linear. The cost is now
proportional to the number of transmitted spikes.
(N.B.: actually if we can do that this could make a short paper).

Again, we need 1) local synaptic variables, 2) event-driven updates.
In addition, we need to be able to modulate synaptic weights with
synaptic variables (perhaps implement a synaptic reset: "x=x+(1-g)*w").

There may also be yet another trick using a variable representing xg
(approximation only).

Thinking about it, we may be able to include everything (STDP, STP, nonlinear
conductances) in the same setting.

Proposition: a new Connection class
-----------------------------------
In all these cases, we need to be able to have local synaptic variables.
These cannot be
defined in the neuron group since it will depend on the connection.
So they have to defined for a specific connection object, either when the
Connection is created, or after. Then we need to define what happens when
a presynaptic/postsynaptic spike occurs. I suggest that we define a new
Connection object which will handle the most general cases. The underlying
structure will be sparse. This is the most general case and it does not
incur a very strong penalty in the specific cases (dense connections).
If we can define variables (possibly with equations)
and what happens at pre/post times, then we can handle pretty much
everything, including the problem of probabilistic synapses. We also need
to have pre and post synaptic delays.

This should resemble the current STDP class, with equations:
eqs="""
dg/dt=-a*g+b*x*(1-g) : 1
dx/dt=-c*x : 1
w : 1 # synaptic weight
"""
where all variables are considered synaptic.

We could define parameters there, just in the same way as a NeuronGroup.
We then need to define what happens with pre and post-synaptic spikes:
pre="""
x -> x+w # transmission of spikes
# here we can have STP event-driven updates
"""
post="""
# here we can have STDP event-driven updates
"""
This could include event-driven updates, if t and tpre/tpost (last update
times) are in the namespace.
Other example:
pre="v -> v+w"
When a variable is not defined, we look for it in the postsynaptic group.
We could also include rand() to have probabilistic synapses.
We also need to add lumped variables (sum of variables, to be updated
at every timestep). There are several possibilities for the syntax.
We could use something like linked_var, for example:
P.g=lumped_var(synapses.g)
where P is the NeuronGroup, and g must have been defined as a parameter
in P.

Having everything synaptic in the same object
might make code generation (C/GPU) easier too, because we will only need
to port one class, which includes spike propagation, STDP and STP. It should
also make it more efficient in this case.

This calls for a complete rewrite of the Connection class (which could have
a different name). Let's call it Synapses for now. We can keep the current
classes for the underlying structures (ConnectionMatrix etc).

We will need to rethink:
* Heterogeneous delays (and delays should be handled as local variables). We
  may need event queues.
* connect methods (now that we may have many variables)
* Accessing weights is replaced by accessing synaptic variables: just as in
  a NeuronGroup, except we have a two dimensional index.
* Including multiple synapses.
* Extending StateMonitor to Synapses.
* Event-driven updates: this is easy if the user does it explicitly (just some
  custom code as in STDP pre/post). This is more complicated if we want to do
  this automatically from the differential equations (but it's possible).

I think we can reuse lots of things from NeuronGroup. We could actually have
a state matrix as in NeuronGroup. This way we can also use StateMonitor.

So my view is that the Synapses class should be a mix between Connection and
NeuronGroup (this way we get StateMonitor for almost free).
In fact, the Synapse class could inherit from both these classes (I would think
it is better to do it from scratch, but perhaps it would save some time).

Main things to think about at this point:
* Nice syntax
* How to handle delays in an efficient way
