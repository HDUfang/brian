BEP-21-Synaptic dynamics

Abstract:
	Currently, synaptic dynamics are only handled at the pre or post-synaptic
	neuron, rather than at the local synapse. For example, for a given
	postsynaptic neuron, there is a single
	equation for all synapses with the same dynamics (e.g. AMPA excitatory
	current) and all presynaptic spikes impact the same variable. In STDP,
	synaptic variables are considered as either presynaptic or postsynaptic,
	not specific of each synapse.
	This design implies strong constraint on synaptic models (kinetics
	and plasticity).
	This BEP aims at overcoming these limitations.

Missing features and a few ideas
================================
* Nonlinear synapses (e.g. NMDA). These require one conductance variable per
  synapse, and separate dynamics. The idea is to
  define differential equations on synaptic weights.
  We could use a fake NeuronGroup as a proxy to synaptic weights + state updater.
  We could use a slower clock to speed up the simulation.
  Issues: 1) several synaptic variables, 2) heterogeneous delays, 3) routing
  spikes (now presynaptic neurons act on different variables).
  A priori, it is not possible to do event-driven dynamics, because of the
  interaction of the conductances with the membrane equation. But maybe there is
  a trick to be found.

* Probabilistic synapses.
  The main difficulty here is to integrate it with STP and STDP.

* Multiple synapses (with different delays) between the same pre/post neurons
  This is almost done, except we need to check if it works with STDP.
  We also need to think of a nice syntax.
  
* STDP: access to postsynaptic variables
  This is probably not a major problem (and can be done with the current version,
  although not in an elegant way), unless it is coupled with local synaptic
  variables. We have one example (Fusi's STDP rule) from people in the mailing
  list.

* STDP: rules with local synaptic variables
  Here we need event-driven dynamics. But one issue is dealing with heterogeneous
  delays. We should start with an existing example (STDP1/2). Then the most
  interesting rule is Graupner/Brunel's.
  
* Neuromodulation. There is a question on this in the Brian support list. This
  is also something that NEST supports. There are two issues here: 1) continuous
  weight dynamics, which should be computed with event-driven updates, 2)
  modulation by an external signal, which is a trace of spikes from other neurons.
  At this point, I'm familiar enough with this to address point 2.

The main technical difficulty, I think, is dealing with heterogeneous delays,
which might require event queues.

Multiple synapses
-----------------
Having multiple synapses (for a given pre/post pair) is entirely possible
with the current sparse connection structure (although STDP should be checked).
What is really needed is a nice syntax to create these synapses and
access the weights.

Nonlinear synapses
------------------
The classic example is NMDA synapses.
Reference:
Probabilistic decision making by slow reverberation in cortical circuits.
XJ Wang, Neuron 2002.

Model:
dg/dt=-a*g+b*x*(1-g)
dx/dt=-c*x
and spikes act on x (addition).

The conductance then enters the membrane equation.
The problem here is that it is a nonlinear system (product of x and g),
and therefore we cannot simply simulate the total conductance, as we do
with linear synapses ("lumped variable"). Each synapse must be individually
simulated. This implies a huge computational cost (proportional to
number of synapses/dt for each second). It cannot be event-driven because the
value of the total conductance is needed at each time step.

There is an example in the support list, where people used a NetworkOperation
to do it.

Tricks
^^^^^^
Example x(t) should be close to 0 most of the time, because 1/c represents the
rise time, which is short. When x=0, the equations are linear (linear decay
of g), and therefore we only need to simulate the lumped variable gtot(t)
corresponding to the sum of all conductances for which x=0. This could however be
quite painful to code.

Another approach is to use an approximation of NMDA dynamics:
dg/dt=-a*g+b*x
dx/dt=-c*x
and x->x+(1-g)*w

This makes sense because g(t) changes slowly relatively to x(t).
If we do that, the equations are linear and we only need to simulate
the lumped variables. However, we still need individual synaptic values
for g (not lumped) at spike time. Here we could use event-driven updates
of g - this is possible because the equations are linear. The cost is now
proportional to the number of transmitted spikes.
(N.B.: actually if we can do that this could make a short paper).

Again, we need 1) local synaptic variables, 2) event-driven updates.
In addition, we need to be able to modulate synaptic weights with
synaptic variables (perhaps implement a synaptic reset: "x=x+(1-g)*w").

There may also be yet another trick using a variable representing xg
(approximation only).

Thinking about it, we may be able to include everything (STDP, STP, nonlinear
conductances) in the same setting.

Proposition: a new Connection class
-----------------------------------
In all these cases, we need to be able to have local synaptic variables.
These cannot be
defined in the neuron group since it will depend on the connection.
So they have to defined for a specific connection object, either when the
Connection is created, or after. Then we need to define what happens when
a presynaptic/postsynaptic spike occurs. I suggest that we define a new
Connection object which will handle the most general cases. The underlying
structure will be sparse. This is the most general case and it does not
incur a very strong penalty in the specific cases (dense connections).
If we can define variables (possibly with equations)
and what happens at pre/post times, then we can handle pretty much
everything, including the problem of probabilistic synapses. We also need
to have pre and post synaptic delays.

This should resemble the current STDP class, with equations:
eqs="""
dg/dt=-a*g+b*x*(1-g) : 1
dx/dt=-c*x : 1
w : 1 # synaptic weight
"""
where all variables are considered synaptic.

We could define parameters there, just in the same way as a NeuronGroup.
We then need to define what happens with pre and post-synaptic spikes:
pre="""
x -> x+w # transmission of spikes
# here we can have STP event-driven updates
"""
post="""
# here we can have STDP event-driven updates
"""
This could include event-driven updates, if t and tpre/tpost (last update
times) are in the namespace.
Other example:
pre="v -> v+w"
When a variable is not defined, we look for it in the postsynaptic group.
We could also include rand() to have probabilistic synapses.
We also need to add lumped variables (sum of variables, to be updated
at every timestep). There are several possibilities for the syntax.
We could use something like linked_var, for example:
P.g=lumped_var(synapses.g)
where P is the NeuronGroup, and g must have been defined as a parameter
in P.

Having everything synaptic in the same object
might make code generation (C/GPU) easier too, because we will only need
to port one class, which includes spike propagation, STDP and STP. It should
also make it more efficient in this case.

This calls for a complete rewrite of the Connection class (which could have
a different name). Let's call it Synapses for now. We can keep the current
classes for the underlying structures (ConnectionMatrix etc).

We will need to rethink:
* Heterogeneous delays (and delays should be handled as local variables). We
  may need event queues.
* connect methods (now that we may have many variables)
* Accessing weights is replaced by accessing synaptic variables: just as in
  a NeuronGroup, except we have a two dimensional index.
* Including multiple synapses.
* Extending StateMonitor to Synapses.
* Event-driven updates: this is easy if the user does it explicitly (just some
  custom code as in STDP pre/post). This is more complicated if we want to do
  this automatically from the differential equations (but it's possible).

I think we can reuse lots of things from NeuronGroup. We could actually have
a state matrix as in NeuronGroup. This way we can also use StateMonitor.

So my view is that the Synapses class should be a mix between Connection and
NeuronGroup (this way we get StateMonitor for almost free).
In fact, the Synapse class could inherit from both these classes (I would think
it is better to do it from scratch, but perhaps it would save some time).

Main things to think about at this point:
* Nice syntax
* How to handle delays in an efficient way

My post to the development mailing list:
----------------------------------------
Hi everyone,

On the support mailing list and in personal conversations, we have seen a lot of interest from users to simulate synaptic models that are currently difficult to do with Brian. These include:
* NMDA (nonlinear) synapses
* probabilistic synapses
* a number of STDP rules with local synaptic variables, requiring event-driven updates

I think the most important issue is the last one, because most recent STDP rules require that and there is no simple trick.
Currently, Brian can only simulate STDP rules where synaptic variables are separable between pre and post, that is, presynaptic variables are simulated (clock-driven) on the presynaptic side, postsynaptic variables on the postsynaptic side. This is efficient because the cost scales as the number of neurons rather than the number of synapses. But it puts strong restrictions on the kind of models we can simulate. There is a trick to do event-driven updates with local synaptic variables, that I posted on the main support list, but I think it's time to have a more general and elegant solution.

My proposition is that we have a new Connection class, which I'll call Synapses for now, which all handle everything synaptic, kinetics and plasticity (long-term and short-term). It will use concepts from NeuronGroup and STDP, that is, you define synaptic variables (including the weights), possibly with their dynamics, and what happens at the time of pre and post synaptic spikes. I wrote a few ideas in BEP-21.

Example 1:
S=Synapses(source,target,eqs="w : 1",pre="v->v+w")

This would be equivalent to S=Connection(source,target,'v'). In the pre code, all the variables not defined in the equations are first looked for in the postsynaptic group (then in the global namespace if not found). As in STDP, the pre code would be internally vectorized. In the namespace of the user code, we would add t, the clock time, and t_pre/t_post, which would hold the time of the previous pre/post spikes. This will enable event-driven code. If we add a post code, then we can write anything that is currently done by STP and STDP.
In the equations, we may have anything that is in NeuronGroup, such as differential equations. For now, these could be clock-driven simulated, but we can also imagine to have an automatic event-driven solver for linear equations.

The variables in this class could be accessed as in NeuronGroup, that is: S.w. The only difference is the indexing (which is 2D instead of 1D). Internally, we could still have a state matrix with the value of all variables (one row = one variable, stored as a vector), plus a variable with the connection structure (target neurons). We can use StateMonitor on these objects.
This will require to change the current ConnectionMatrix structures to handle multiple variables.

This has a number of advantages. It is general and flexible, it really extends the number of things you will be able to simulate with Brian. It could be more efficient, because it minimizes the number of objects. It will make it easier to port to GPU as it's a single object.

The main technical problem is see is with heterogeneous delays. The current solution will not work because it relies on the idea that all presynaptic spikes act on the same variable. What we need is really to execute the pre and post code at the time of spikes, and it could be tricky to vectorize. One possibility is to have event queues. The main change is that we now won't be able to vectorize over contiguous rows of the connection matrix. But maybe it's not such a big issue, because this is what happens already with STDP (for postsynaptic spikes). The main difficulty I see with implementing an efficient event queue is when a postsynaptic neuron receives simultaneous events.

So I would say we first need to solve the problem of delays!

Romain

