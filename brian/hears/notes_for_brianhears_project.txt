Brianhears
===========
Brianhears is an auditory library for Brian.

Example 1
=========
sound = Sound('filename.wav')
defaultclock.dt = 1/sound.samplerate 
fb = GammatoneFilterbank(erbspace(20*Hz, 20*kHz, 3000), samplerate=sound.samplerate, order=4, ...)
#G = FilterbankGroup(fb,sound)
#G = GammatoneGroup(erbspace(20*Hz, 20*kHz, 3000), sound=sound, order=4, ...)

neurons=NeuronGroup(len(G),model='''dv/dt=(I-v)/(5*ms):1
                                    I:1''',...)
neurons.I=linked_var(G,'output')

run(1*second)
G.load_sound(sound2) # G.load(sound2) ?
# G.input=sound2 ?
run(1*second)

Example 2
=========
sound = Sound('filename.wav')
defaultclock.dt = 1/sound.samplerate 
# Middle ear filtering (maybe on CPU; GPU: FFT-based?)
filtered1 = Filter(sound)
filtered2 = Filter(filtered1)
# Filter banks
fb = GammatoneFilterbank(erbspace(20*Hz, 20*kHz, 3000), samplerate=sound.samplerate, order=4, sound=filtered2)
fb2 = SomeOtherFilterbank(erbspace(20*Hz, 20*kHz, 3000), samplerate=sound.samplerate, order=4, sound=fb)
# Second pathway
fb3 = GammatoneFilterbank(erbspace(20*Hz, 20*kHz, 3000), samplerate=sound.samplerate, order=4, sound=filtered2)
fb4 = Compression(N=3000,args=...,sound=fb3)
# Addition
output = Addition(fb2,fb4) # fb2+fb4
G = FilterbankGroup(output, sound) # gathers all code

soundproducer = SoundProducer(sound)
gammatone = Gammatone(soundproducer)
spikes = Spikes(gammatone)

Linked variables
----------------
Connecting var x in G to var y in H via function f optionally:
H.y = linked_var(G, 'x', func=f, when='start', clock=H.clock)

Existing libraries
==================
http://www.pdn.cam.ac.uk/groups/dsam/
http://www.pdn.cam.ac.uk/groups/cnbh/aimmanual/index.html
http://cobweb.ecn.purdue.edu/~malcolm/interval/1998-010/
http://www.essex.ac.uk/psychology/psy/PEOPLE/meddis/webFolder08/WebIntro.htm
http://www.urmc.rochester.edu/labs/Carney-Lab/publications/auditory-models.cfm
http://www.pdn.cam.ac.uk/groups/cnbh/aimmanual/index.html


References
==========
A good review of physiological models (check Fig.6 for the global structure): 
Int Rev Neurobiol. 2005;70:7-48.
Spectral processing by the peripheral auditory system: facts and models.
Lopez-Poveda EA.
http://www.ncbi.nlm.nih.gov/pubmed/16472630

Spirit of Brianhears
====================
In existing libraries, sound generation and filtering are done offline, i.e., functions are
applied to the entire sound. It takes a lot of memory and is not adapted to Brian. In Brianhears,
we need online sound generation and filtering. Besides, for efficiency, operations need to be
vectorised across channels.
In most auditory models, there are two types of filtering: global (external and middle ear)
and local (inner ear, one filter per channel). For each channel, there are one or two pathways,
each pathway is a chain of operations (filter/rectification/compression). These operations are
identical for all channels, except the parameters are different. This arrangement
may be duplicated for stereo models. For optimal efficiency, simulation should be vectorised
also over the two ears (left/right).

Global operations
-----------------
The most efficient way to do global filtering or other operations is to vectorise over time
by using a buffer mechanism: output is delayed by time T and input is processed by blocks of T.

What operations?
* linear filters
* compression
* pre-cochlear processing described in http://www.pdn.cam.ac.uk/groups/cnbh/aimmanual/PCP/PCPframeset.htm

Local operations
----------------
IIR filters (e.g. gammatone) can often be implemented as linear differential systems,
with discrete-time updates (matrix product). This is the simplest approach with Brian.
If this is not possible, we have two options:
* approximate the filter by a finite-dimensional linear filter;
* use convolutions, and possibly vectorise over time using buffering.

What filters:
* Gammatone
* Gammachirp
* Butterworth
* Lyon's cochlear model
* Hair cell / Auditory nerve models? (e.g. Meddis)

GPU implementation
==================
Local operations should be very easy to parallelize. For optimal efficiency (using shared
memory etc), it is likely that using buffering would help (i.e., many timesteps are computed
within the same kernel execution). The whole chain can probably be simulated on GPU.

Spike production
----------------
For optimal efficiency, spikes should be produced on the GPU and transferred to the CPU,
without transferring anything else (smaller bandwidth consumption). Thus the filterbank
code should be interfaced with generic GPU features of Brian.

Other features
==============
* general sound manipulation functions
* online/offline sound generation (including stereo)
* analysis of HRTF databases (e.g. ITD/ILD estimation)
* compression/rectification

HRTF module
===========
It would be good to have a set of functions to load and analyse HRTF data.
A good way to do that would be to have HRTFSet class, which would contain and manage
a complete set of HRTFs (class HRTF).

HRTF
----
This is basically two arrays with some general information (sampling frequency).
* Binaural information: phase, ITD and ILD with several methods (frequency-dependent)
** hrtf.ITD(freq1,freq2,method="MaxICC")
** hrtf.phase(freq) or hrtf.phase() # all frequencies
* Fourier domain / time domain conversion
* hrtf.left, hrtf.right (the two impulse responses, as Sound objects)

The HRTF class could derive from a stereo Sound class.

HRTFSet
-------
* Loading
* Coordinate conversion
* hrtf[azimuth,elevation] returns the HRTF for the given direction
* Slices: hrtf[a1:a2,e1:e2]
* Maybe interpolation (I would say: interpolate in the Fourier domain, then convert to time domain)
* Artificial HRTFs (sphere etc?)
* Simultaneous calculations (all directions): ITD, etc
